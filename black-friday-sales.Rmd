---
title: "Black-Friday Sales Prediction"
output: html_notebook
---

> Contest Link: https://datahack.analyticsvidhya.com/contest/black-friday/

### Problem Statement

A retail company “ABC Private Limited” wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.
The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.

Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products.

### Solution

The dataset can be downloaded from [contest link](https://datahack.analyticsvidhya.com/contest/black-friday/). 

- Lets load the train and test dataset into the dataframe. As we see, header is present in the dataset, lets pass with the argument, `header=TRUE` to skip the first row.

```{r}
train.df <- read.csv("train.csv", header = TRUE)
test.df <- read.csv("test.csv", header = TRUE) 
```

- Starting with priliminary analysis, we first check the dimension of the data

```{r}
dim(train.df)
dim(test.df)
```

- Secondly, we run the below to check for missing values. Missing values can cause problems in the training phase if not taken care of before.

```{r}
any(is.na(train.df))
```

- Checking which columns, the missing values belong to

```{r}
names(which(sapply(train.df, anyNA)))
```

- Lets see the summary of each dataframes to get the count of the missing values in the training and test set. Summary provides an overall insight into the data.

```{r}
summary(train.df)
summary(test.df)
```

- Imputing the missing value with 20 in the training data

```{r}
train.df[10][is.na(train.df[10])] <- 20
train.df[11][is.na(train.df[11])] <- 20
```

- Imputing the missing value with 20, in the testing data

```{r}
test.df[10][is.na(test.df[10])] <- 20
test.df[11][is.na(test.df[11])] <- 20
```

- Checking the training and testing data to make sure there are no more missing values

```{r}
sum(is.na(train.df))
sum(is.na(test.df))
```


```{r}
train$data <- 1
test$Purchase <- 0
test$data <- 0
```

```{r}
total <- rbind(train,test)
```

```{r}
for (i in 1:11)
{
  total[,i] <- as.factor(total[,i])
}
```

```{r}
train.notmar <- total[total$Marital_Status == 0 ,] 
train.notmar <- train.notmar[train.notmar$data == 1,]
test.notmar <- total[total$data == 0 ,]
```

```{r}
test.notmar$Purchase <- NULL
test.notmar$data <- NULL
train.notmar$data <- NULL
```

- Decision Tree

```{r}
library(rpart)
```

```{r}
model <- rpart(Purchase ~ .,data = train.notmar)
pred_tree <- predict(model, test.notmar)
```

```{r}
submit <- data.frame(User_ID = test$User_ID,
                     Product_ID = test$Product_ID,
                     Purchase = pred_tree)
```

- XGboost

```{r}
library(xgboost)
```

```{r}
for (i in 1:12)
{
  train.notmar[,i] <-  as.numeric(train.notmar[,i])
}
```

```{r}
for (i in 1:11)
{
  test.notmar[,i] <-  as.numeric(test.notmar[,i])
}

```

- Features

```{r}
X_features <- c( "User_ID" , "Product_ID" , "Gender" ,                   
                 "Age" , "Occupation" ,  "City_Category" ,           
                 "Stay_In_Current_City_Years" , "Product_Category_1" ,       
                 "Product_Category_2" , "Product_Category_3")
```

```{r}
X_target <- train.notmar$Purchase
```

```{r}
xgtrain <- xgb.DMatrix(data <- as.matrix(train.notmar[, X_features]), label = X_target, missing = NA)
xgtest <- xgb.DMatrix(data <- as.matrix(test.notmar[, X_features]), missing = NA)
```

- Setting Parameters
```{r}
params <- list()
params$objective <- "reg:linear"
params$eta <- 0.23
params$max_depth <- 10
params$subsample <- 1
params$colsample_bytree <- 1
params$min_child_weight <- 2
params$eval_metric <- "rmse"
```

- building model

```{r}
model_xgb <- xgb.train(params <- params, xgtrain, nrounds <- 100)
```

- checking important Features

```{r}
vimp <- xgb.importance(model <- model_xgb, feature_names = X_features)
```

- Predicting

```{r}
pred_boost <- predict(model_xgb, xgtest)
```

- Submission

```{r}
submit$Purchase_boosted <- pred_boost 
Final_submit <- submit
```

- Weighted Average of Decision tree and Boosting

```{r}
Final_submit<-Final_submit[,-c(4)]
Final_submit$Purchase_1 <- (submit$Purchase + submit$Purchase_boosted)/2
write.csv(Final_submit[,-c(3,4)], "result.csv", row.names = FALSE)
```